<html>
<body>
<h1>GUI-editable parameters</h1>
<h2>Regular classes</h2>
<h3>CACLALearner</h3>
<ul>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : Learning gain [0..1]</li>
<li><b>Policy</b> (<i>Policy subclass</i>) : The policy to be learned</li>
</ul>
<h3>IncrementalNaturalActorCritic</h3>
<ul>
<li><b>VFunction</b> (<i>LinearStateVFA</i>) : The Value-function</li>
<li><b>V-ETraces</b> (<i>ETraces</i>) : Traces used by the critic</li>
<li><b>Alpha-v</b> (<i>NumericValue subclass</i>) : Learning gain used by the critic</li>
<li><b>Alpha-r</b> (<i>NumericValue subclass</i>) : Learning gain used to average the reward</li>
<li><b>Alpha-u</b> (<i>NumericValue subclass</i>) : Learning gain used by the actor</li>
<li><b>Policy</b> (<i>Multiple Policy subclass</i>) : The policy</li>
</ul>
<h3>OffPolicyActorCritic</h3>
<ul>
<li><b>VFunction</b> (<i>LinearStateVFA</i>) : The Value-function</li>
<li><b>ETraces</b> (<i>ETraces</i>) : Traces used by the critic and the actor</li>
<li><b>Alpha-v</b> (<i>NumericValue subclass</i>) : Learning gain used by the critic</li>
<li><b>Alpha-w</b> (<i>NumericValue subclass</i>) : Learning gain used to average the reward</li>
<li><b>Alpha-u</b> (<i>NumericValue subclass</i>) : Learning gain used by the actor</li>
<li><b>Policy</b> (<i>Multiple Policy subclass</i>) : The policy</li>
</ul>
<h3>OffPolicyDeterministicActorCritic</h3>
<ul>
<li><b>QFunction</b> (<i>LinearStateActionVFA</i>) : The Q-function</li>
<li><b>Alpha-w</b> (<i>NumericValue subclass</i>) : Learning gain used by the critic</li>
<li><b>Alpha-theta</b> (<i>NumericValue subclass</i>) : Learning gain used by the actor</li>
<li><b>Policy</b> (<i>Multiple DeterministicPolicy subclass</i>) : The deterministic policy</li>
</ul>
<h3>ActorCritic</h3>
<ul>
<li><b>Actor</b> (<i>Actor</i>) : The actor</li>
<li><b>Critic</b> (<i>ICritic subclass</i>) : The critic</li>
</ul>
<h3>RegularPolicyGradientLearner</h3>
<ul>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : The learning gain</li>
<li><b>Policy</b> (<i>Policy subclass</i>) : The policy to be learned</li>
</ul>
<h3>Actor</h3>
<ul>
<li><b>Base-Controller</b> (<i>Controller subclass</i>) : The base controller used to initialize the weights of the actor</li>
<li><b>Output</b> (<i>Multiple PolicyLearner subclass</i>) : The outputs of the actor. One for each output dimension</li>
</ul>
<h3>SimionApp</h3>
<ul>
<li><b>Log</b> (<i>Logger</i>) : The logger class</li>
<li><b>World</b> (<i>World</i>) : The simulation environment and its parameters</li>
<li><b>Experiment</b> (<i>Experiment</i>) : The parameters of the experiment</li>
<li><b>SimGod</b> (<i>SimGod</i>) : The omniscient class that controls all aspects of the simulation process</li>
</ul>
<h3>AsyncQLearning</h3>
<ul>
<li><b>i-async-update</b> (<i>int</i>) : Steps before the network's weights are updated by the optimizer with the accumulated DeltaTheta</li>
<li><b>i-target</b> (<i>int</i>) : Steps before the target network's weights are updated</li>
<li><b>neural-network</b> (<i>NN Definition</i>) : Neural Network Architecture</li>
<li><b>experience-replay</b> (<i>ExperienceReplay</i>) : Experience replay</li>
<li><b>Policy</b> (<i>DiscreteDeepPolicy subclass</i>) : The policy</li>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
</ul>
<h3>LQRGain</h3>
<ul>
<li><b>Gain</b> (<i>double</i>) : The gain applied to the input state variable</li>
<li><b>Variable</b> (<i>State</i>) : The input state variable</li>
</ul>
<h3>LQRController</h3>
<ul>
<li><b>Output-Action</b> (<i>Action</i>) : The output action</li>
<li><b>LQR-Gain</b> (<i>Multiple LQRGain</i>) : An LQR gain on an input state variable</li>
</ul>
<h3>PIDController</h3>
<ul>
<li><b>KP</b> (<i>NumericValue subclass</i>) : Proportional gain</li>
<li><b>KI</b> (<i>NumericValue subclass</i>) : Integral gain</li>
<li><b>KD</b> (<i>NumericValue subclass</i>) : Derivative gain</li>
<li><b>Input-Variable</b> (<i>State</i>) : The input state variable</li>
<li><b>Output-Action</b> (<i>Action</i>) : The output action</li>
</ul>
<h3>WindTurbineVidalController</h3>
<ul>
<li><b>A</b> (<i>NumericValue subclass</i>) : A parameter of the torque controller</li>
<li><b>K_alpha</b> (<i>NumericValue subclass</i>) : K_alpha parameter of the torque controller</li>
<li><b>KP</b> (<i>NumericValue subclass</i>) : Proportional gain of the pitch controller</li>
<li><b>KI</b> (<i>NumericValue subclass</i>) : Integral gain of the pitch controller</li>
</ul>
<h3>WindTurbineBoukhezzarController</h3>
<ul>
<li><b>C_0</b> (<i>NumericValue subclass</i>) : C_0 parameter</li>
<li><b>KP</b> (<i>NumericValue subclass</i>) : Proportional gain of the pitch controller</li>
<li><b>KI</b> (<i>NumericValue subclass</i>) : Integral gain of the pitch controller</li>
</ul>
<h3>WindTurbineJonkmanController</h3>
<ul>
<li><b>CornerFreq</b> (<i>double</i>) : Corner Freq. parameter</li>
<li><b>VS_SlPc</b> (<i>double</i>) : SIPc parameter</li>
<li><b>VS_Rgn2K</b> (<i>double</i>) : Rgn2K parameter</li>
<li><b>VS_Rgn2Sp</b> (<i>double</i>) : Rgn2Sp parameter</li>
<li><b>VS_CtInSp</b> (<i>double</i>) : CtlnSp parameter</li>
<li><b>VS_Rgn3MP</b> (<i>double</i>) : Rgn3MP parameter</li>
<li><b>PC_RefSpd</b> (<i>double</i>) : Pitch control reference speed</li>
<li><b>PC_KK</b> (<i>NumericValue subclass</i>) : Pitch angle were the the derivative of the...</li>
<li><b>PC_KP</b> (<i>NumericValue subclass</i>) : Proportional gain of the pitch controller</li>
<li><b>PC_KI</b> (<i>NumericValue subclass</i>) : Integral gain of the pitch controller</li>
</ul>
<h3>Controller-Factory</h3>
<ul>
<li><b>Controller</b> (<i>Controller subclass</i>) : The specific controller to be used</li>
</ul>
<h3>TDLambdaCritic</h3>
<ul>
<li><b>E-Traces</b> (<i>ETraces</i>) : Eligibility traces of the critic</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : Learning gain</li>
<li><b>V-Function</b> (<i>LinearStateVFA</i>) : The V-function to be learned</li>
</ul>
<h3>TDCLambdaCritic</h3>
<ul>
<li><b>E-Traces</b> (<i>ETraces</i>) : Elilgibility traces of the critic</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : Learning gain of the critic</li>
<li><b>Beta</b> (<i>NumericValue subclass</i>) : Learning gain applied to the omega vector</li>
<li><b>V-Function</b> (<i>LinearStateVFA</i>) : The V-function to be learned</li>
</ul>
<h3>TrueOnlineTDLambdaCritic</h3>
<ul>
<li><b>E-Traces</b> (<i>ETraces</i>) : Eligibility traces of the critic</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : Learning gain of the critic</li>
<li><b>V-Function</b> (<i>LinearStateVFA</i>) : The V-function to be learned</li>
</ul>
<h3>VLearnerCritic</h3>
<ul>
<li><b>V-Function</b> (<i>LinearStateVFA</i>) : The V-function to be learned</li>
</ul>
<h3>ICritic-Factory</h3>
<ul>
<li><b>Critic</b> (<i>ICritic subclass</i>) : Critic type</li>
</ul>
<h3>DDPG</h3>
<ul>
<li><b>Tau</b> (<i>double</i>) : The rate by which the target weights approach the online weights</li>
<li><b>Learning-Rate</b> (<i>double</i>) : The learning rate at which the agent learns</li>
<li><b>Critic-Network</b> (<i>NN Definition</i>) : Neural Network for the Critic -a Q function-</li>
<li><b>Actor-Network</b> (<i>NN Definition</i>) : Neural Network for the Actor -deterministic policy-</li>
<li><b>Exploration-Noise</b> (<i>Noise subclass</i>) : Noise added to the output of the policy</li>
<li><b>Input-State</b> (<i>Multiple State</i>) : Set of state variables used as input</li>
<li><b>Output-Action</b> (<i>Multiple Action</i>) : The output action variable</li>
</ul>
<h3>DiscreteEpsilonGreedyDeepPolicy</h3>
<ul>
<li><b>epsilon</b> (<i>NumericValue subclass</i>) : Epsilon</li>
</ul>
<h3>DiscreteSoftmaxDeepPolicy</h3>
<ul>
<li><b>temperature</b> (<i>NumericValue subclass</i>) : Tempreature</li>
</ul>
<h3>DiscreteDeepPolicy-Factory</h3>
<ul>
<li><b>Policy</b> (<i>DiscreteDeepPolicy subclass</i>) : The policy type</li>
</ul>
<h3>DQN</h3>
<ul>
<li><b>Num-Action-Steps</b> (<i>int</i>) : Number of discrete values used for the output action</li>
<li><b>Learning-Rate</b> (<i>double</i>) : The learning rate at which the agent learns</li>
<li><b>neural-network</b> (<i>NN Definition</i>) : Neural Network Architecture</li>
<li><b>Policy</b> (<i>DiscreteDeepPolicy subclass</i>) : The policy</li>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
<li><b>Input-State</b> (<i>Multiple State</i>) : Set of variables used as input of the QNetwork</li>
</ul>
<h3>DoubleDQN</h3>
<ul>
<li><b>Num-Action-Steps</b> (<i>int</i>) : Number of discrete values used for the output action</li>
<li><b>Learning-Rate</b> (<i>double</i>) : The learning rate at which the agent learns</li>
<li><b>neural-network</b> (<i>NN Definition</i>) : Neural Network Architecture</li>
<li><b>Policy</b> (<i>DiscreteDeepPolicy subclass</i>) : The policy</li>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
<li><b>Input-State</b> (<i>Multiple State</i>) : Set of variables used as input of the QNetwork</li>
</ul>
<h3>ETraces</h3>
<ul>
<li><b>Threshold</b> (<i>double</i>) : Threshold applied to trace factors</li>
<li><b>Lambda</b> (<i>double</i>) : Lambda parameter</li>
<li><b>Replace</b> (<i>bool</i>) : Replace existing traces? Or add?</li>
</ul>
<h3>ExperienceReplay</h3>
<ul>
<li><b>Buffer-Size</b> (<i>int</i>) : Size of the buffer used to store experience tuples</li>
<li><b>Update-Batch-Size</b> (<i>int</i>) : Number of tuples used each time-step in the update</li>
</ul>
<h3>Experiment</h3>
<ul>
<li><b>Random-Seed</b> (<i>int</i>) : Random seed used to generate random sequences of numbers</li>
<li><b>Num-Episodes</b> (<i>int</i>) : Number of episodes. Zero if we only want to run one evaluation episode</li>
<li><b>Eval-Freq</b> (<i>int</i>) : Evaluation frequency (in episodes). If zero then only training episodes will be run</li>
<li><b>Progress-Update-Freq</b> (<i>double</i>) : Progress update frequency (seconds)</li>
<li><b>Episode-Length</b> (<i>double</i>) : Length of an episode(seconds)</li>
</ul>
<h3>TileCodingFeatureMap</h3>
<ul>
<li><b>Num-Tiles</b> (<i>int</i>) : Number of tile layers of the grid</li>
<li><b>Tile-Offset</b> (<i>double</i>) : Offset of each tile relative to the previous one. It is scaled by the value range of the input variable</li>
</ul>
<h3>FeatureMap</h3>
<ul>
<li><b>Num-Features-Per-Dimension</b> (<i>int</i>) : Number of features per input variable</li>
</ul>
<h3>StateFeatureMap</h3>
<ul>
<li><b>Feature-Mapper</b> (<i>FeatureMapper subclass</i>) : The feature calculator used to map/unmap features</li>
<li><b>Input-State</b> (<i>Multiple State</i>) : State variables used as input of the feature map</li>
<li><b>Num-Features-Per-Dimension</b> (<i>int</i>) : Number of features per input variable</li>
</ul>
<h3>ActionFeatureMap</h3>
<ul>
<li><b>Feature-Mapper</b> (<i>FeatureMapper subclass</i>) : The feature calculator used to map/unmap features</li>
<li><b>Input-Action</b> (<i>Multiple Action</i>) : Action variables used as input of the feature map</li>
<li><b>Num-Features-Per-Dimension</b> (<i>int</i>) : Number of features per input variable</li>
</ul>
<h3>FeatureMapper-Factory</h3>
<ul>
<li><b>Type</b> (<i>FeatureMapper subclass</i>) : Feature map type</li>
</ul>
<h3>Logger</h3>
<ul>
<li><b>Num-Functions-Logged</b> (<i>int</i>) : How many times per experiment save logged functions</li>
<li><b>Log-Freq</b> (<i>double</i>) : Log frequency. Simulation time in seconds.</li>
<li><b>Log-Eval-Episodes</b> (<i>bool</i>) : Log evaluation episodes?</li>
<li><b>Log-Training-Episodes</b> (<i>bool</i>) : Log training episodes?</li>
<li><b>Log-Functions</b> (<i>bool</i>) : Log functions learned?</li>
</ul>
<h3>GaussianNoise</h3>
<ul>
<li><b>Sigma</b> (<i>double</i>) : Width of the gaussian bell</li>
<li><b>Alpha</b> (<i>double</i>) : Low-pass first-order filter's gain [0...1]. 1=no filter</li>
<li><b>Scale</b> (<i>NumericValue subclass</i>) : Scale factor applied to the noise signal before adding it to the policy's output</li>
</ul>
<h3>SinusoidalNoise</h3>
<ul>
<li><b>Time-Frequency</b> (<i>double</i>) : Frequency of the signal in 1/simulation seconds</li>
<li><b>Amplitude-Scale</b> (<i>NumericValue subclass</i>) : Scaling factor applied to the sinusoidal</li>
</ul>
<h3>OrnsteinUhlenbeckNoise</h3>
<ul>
<li><b>Mu</b> (<i>double</i>) : Mean value of the generated noise</li>
<li><b>Sigma</b> (<i>double</i>) : Degree of volatility around it caused by shocks</li>
<li><b>Theta</b> (<i>double</i>) : Rate by which noise shocks dissipate and the variable reverts towards the mean</li>
<li><b>Scale</b> (<i>NumericValue subclass</i>) : Scale factor applied to the noise signal before adding it to the policy's output</li>
</ul>
<h3>Noise-Factory</h3>
<ul>
<li><b>Noise</b> (<i>Noise subclass</i>) : Noise type</li>
</ul>
<h3>ConstantValue</h3>
<ul>
<li><b>Value</b> (<i>double</i>) : Constant value</li>
</ul>
<h3>SimpleEpisodeLinearSchedule</h3>
<ul>
<li><b>Initial-Value</b> (<i>double</i>) : Value at the beginning of the experiment</li>
<li><b>End-Value</b> (<i>double</i>) : Value at the end of the experiment</li>
</ul>
<h3>InterpolatedValue</h3>
<ul>
<li><b>Start-Offset</b> (<i>double</i>) : Normalized time from which the schedule will begin [0...1]</li>
<li><b>End-Offset</b> (<i>double</i>) : Normalized time at which the schedule will end and only return the End-Value [0...1]</li>
<li><b>Pre-Offset-Value</b> (<i>double</i>) : Output value before the schedule begins</li>
<li><b>Initial-Value</b> (<i>double</i>) : Output value at the beginning of the schedule</li>
<li><b>End-Value</b> (<i>double</i>) : Output value at the end of the schedule</li>
<li><b>Evaluation-Value</b> (<i>double</i>) : Output value during evaluation episodes</li>
<li><b>Interpolation</b> (<i>Interpolation</i>) : Interpolation type</li>
<li><b>Time-reference</b> (<i>TimeReference</i>) : The time-reference type</li>
</ul>
<h3>BhatnagarSchedule</h3>
<ul>
<li><b>Alpha-0</b> (<i>double</i>) : Alpha-0 parameter in Bhatnagar's schedule</li>
<li><b>Alpha-c</b> (<i>double</i>) : Alpha-c parameter in Bhatnagar's schedule</li>
<li><b>Time-Exponent</b> (<i>double</i>) : Time exponent in Bhatnagar's schedule</li>
<li><b>Evaluation-Value</b> (<i>double</i>) : Output value during evaluation episodes</li>
<li><b>Time-reference</b> (<i>TimeReference</i>) : The time reference</li>
</ul>
<h3>WireConnection</h3>
<ul>
<li><b>Wire</b> (<i>Wire</i>) : Wire connection from which the value comes</li>
</ul>
<h3>NumericValue-Factory</h3>
<ul>
<li><b>Schedule</b> (<i>NumericValue subclass</i>) : Schedule-type</li>
</ul>
<h3>PolicyLearner</h3>
<ul>
<li><b>Policy</b> (<i>Policy subclass</i>) : The policy to be learned</li>
</ul>
<h3>PolicyLearner-Factory</h3>
<ul>
<li><b>Policy-Learner</b> (<i>PolicyLearner subclass</i>) : The algorithm used to learn the policy</li>
</ul>
<h3>QEGreedyPolicy</h3>
<ul>
<li><b>Epsilon</b> (<i>NumericValue subclass</i>) : The epsilon parameter that balances exploitation and exploration</li>
</ul>
<h3>QSoftMaxPolicy</h3>
<ul>
<li><b>Tau</b> (<i>NumericValue subclass</i>) : Temperature parameter</li>
</ul>
<h3>GreedyQPlusNoisePolicy</h3>
<ul>
<li><b>Noise</b> (<i>Multiple Noise subclass</i>) : Noise signal added to the typical greedy action selection. The number of noise signals should match the number of actions in the action feature amp</li>
</ul>
<h3>QLearningCritic</h3>
<ul>
<li><b>Q-Function</b> (<i>LinearStateActionVFA</i>) : The parameterization of the Q-Function</li>
<li><b>E-Traces</b> (<i>ETraces</i>) : E-Traces</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : The learning gain [0-1]</li>
</ul>
<h3>QLearning</h3>
<ul>
<li><b>Policy</b> (<i>QPolicy subclass</i>) : The policy to be followed</li>
<li><b>Q-Function</b> (<i>LinearStateActionVFA</i>) : The parameterization of the Q-Function</li>
<li><b>E-Traces</b> (<i>ETraces</i>) : E-Traces</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : The learning gain [0-1]</li>
</ul>
<h3>DoubleQLearning</h3>
<ul>
<li><b>Policy</b> (<i>QPolicy subclass</i>) : The policy to be followed</li>
<li><b>Q-Function</b> (<i>LinearStateActionVFA</i>) : The parameterization of the Q-Function</li>
<li><b>E-Traces</b> (<i>ETraces</i>) : E-Traces</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : The learning gain [0-1]</li>
</ul>
<h3>SARSA</h3>
<ul>
<li><b>Policy</b> (<i>QPolicy subclass</i>) : The policy to be followed</li>
<li><b>Q-Function</b> (<i>LinearStateActionVFA</i>) : The parameterization of the Q-Function</li>
<li><b>E-Traces</b> (<i>ETraces</i>) : E-Traces</li>
<li><b>Alpha</b> (<i>NumericValue subclass</i>) : The learning gain [0-1]</li>
</ul>
<h3>QPolicy-Factory</h3>
<ul>
<li><b>Policy</b> (<i>QPolicy subclass</i>) : The exploration policy used to learn</li>
</ul>
<h3>SimGod</h3>
<ul>
<li><b>Target-Function-Update-Freq</b> (<i>int</i>) : Update frequency at which target functions will be updated. Only used if Freeze-Target-Function=true</li>
<li><b>Gamma</b> (<i>double</i>) : Gamma parameter</li>
<li><b>Freeze-Target-Function</b> (<i>bool</i>) : Defers updates on the V-functions to improve stability</li>
<li><b>Use-Importance-Weights</b> (<i>bool</i>) : Use sample importance weights to allow off-policy learning -experimental-</li>
<li><b>State-Feature-Map</b> (<i>StateFeatureMap</i>) : The state feature map</li>
<li><b>Action-Feature-Map</b> (<i>ActionFeatureMap</i>) : The state feature map</li>
<li><b>Experience-Replay</b> (<i>ExperienceReplay</i>) : The experience replay parameters</li>
<li><b>Simion</b> (<i>Multiple Simion subclass</i>) : Simions: learning agents and controllers</li>
</ul>
<h3>Simion-Factory</h3>
<ul>
<li><b>Type</b> (<i>Simion subclass</i>) : The Simion class</li>
</ul>
<h3>Policy</h3>
<ul>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
</ul>
<h3>DeterministicPolicy</h3>
<ul>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
</ul>
<h3>StochasticPolicy</h3>
<ul>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
</ul>
<h3>DeterministicPolicyGaussianNoise</h3>
<ul>
<li><b>Deterministic-Policy-VFA</b> (<i>LinearStateVFA</i>) : The parameterized VFA that approximates the function</li>
<li><b>Exploration-Noise</b> (<i>Noise subclass</i>) : Parameters of the noise used as exploration</li>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
</ul>
<h3>StochasticGaussianPolicy</h3>
<ul>
<li><b>Mean-VFA</b> (<i>LinearStateVFA</i>) : The parameterized VFA that approximates the function</li>
<li><b>Sigma-VFA</b> (<i>LinearStateVFA</i>) : The parameterized VFA that approximates variance(s)</li>
<li><b>Output-Action</b> (<i>Action</i>) : The output action variable</li>
</ul>
<h3>Policy-Factory</h3>
<ul>
<li><b>Policy</b> (<i>Policy subclass</i>) : The policy type</li>
</ul>
<h3>LinearStateVFA</h3>
<ul>
<li><b>Init-Value</b> (<i>double</i>) : The initial value given to the weights on initialization</li>
</ul>
<h3>LinearStateActionVFA</h3>
<ul>
<li><b>Init-Value</b> (<i>double</i>) : The initial value given to the weights on initialization</li>
</ul>
<h3>World</h3>
<ul>
<li><b>Num-Integration-Steps</b> (<i>int</i>) : The number of integration steps performed each simulation time-step</li>
<li><b>Delta-T</b> (<i>double</i>) : The delta-time between simulation steps</li>
<li><b>Dynamic-Model</b> (<i>DynamicModel subclass</i>) : The dynamic model</li>
</ul>
<h3>DynamicModel-Factory</h3>
<ul>
<li><b>Model</b> (<i>DynamicModel subclass</i>) : The world</li>
</ul>
<h2>Worlds</h2>
<h3>BalancingPole</h3>
<h4>State variables</h4>
<ul>
<li>x</li>
<li>x_dot</li>
<li>theta</li>
<li>theta_dot</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>force</li>
</ul>
<h3>DoublePendulum</h3>
<h4>State variables</h4>
<ul>
<li>theta_1</li>
<li>theta_1-dot</li>
<li>theta_2</li>
<li>theta_2-dot</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>torque_1</li>
<li>torque_2</li>
</ul>
<h3>FASTWindTurbine</h3>
<h4>State variables</h4>
<ul>
<li>T_a</li>
<li>P_a</li>
<li>P_s</li>
<li>P_e</li>
<li>E_p</li>
<li>v</li>
<li>omega_r</li>
<li>d_omega_r</li>
<li>E_omega_r</li>
<li>omega_g</li>
<li>d_omega_g</li>
<li>E_omega_g</li>
<li>beta</li>
<li>d_beta</li>
<li>T_g</li>
<li>d_T_g</li>
<li>E_int_omega_r</li>
<li>E_int_omega_g</li>
<li>theta</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>beta</li>
<li>T_g</li>
</ul>
<h3>MountainCar</h3>
<h4>State variables</h4>
<ul>
<li>position</li>
<li>velocity</li>
<li>height</li>
<li>angle</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>pedal</li>
</ul>
<h3>PitchControl</h3>
<h4>State variables</h4>
<ul>
<li>setpoint-pitch</li>
<li>attack-angle</li>
<li>pitch</li>
<li>pitch-rate</li>
<li>control-deviation</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>pitch</li>
</ul>
<h3>PullBox1</h3>
<h4>State variables</h4>
<ul>
<li>target-x</li>
<li>target-y</li>
<li>robot1-x</li>
<li>robot1-y</li>
<li>box-x</li>
<li>box-y</li>
<li>robot1-theta</li>
<li>box-theta</li>
<li>box-to-target-x</li>
<li>box-to-target-y</li>
<li>robot1-to-box-x</li>
<li>robot1-to-box-y</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>robot1-v</li>
<li>robot1-omega</li>
</ul>
<h3>PullBox2</h3>
<h4>State variables</h4>
<ul>
<li>target-x</li>
<li>target-y</li>
<li>robot1-x</li>
<li>robot1-y</li>
<li>robot2-x</li>
<li>robot2-y</li>
<li>box-x</li>
<li>box-y</li>
<li>robot1-theta</li>
<li>robot2-theta</li>
<li>box-theta</li>
<li>box-to-target-x</li>
<li>box-to-target-y</li>
<li>robot1-to-box-x</li>
<li>robot1-to-box-y</li>
<li>robot2-to-box-x</li>
<li>robot2-to-box-y</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>robot1-v</li>
<li>robot1-omega</li>
<li>robot2-v</li>
<li>robot2-omega</li>
</ul>
<h3>PushBox1</h3>
<h4>State variables</h4>
<ul>
<li>target-x</li>
<li>target-y</li>
<li>robot1-x</li>
<li>robot1-y</li>
<li>box-x</li>
<li>box-y</li>
<li>robot1-to-box-x</li>
<li>robot1-to-box-y</li>
<li>box-to-target-x</li>
<li>box-to-target-y</li>
<li>robot1-theta</li>
<li>box-theta</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>robot1-v</li>
<li>robot1-omega</li>
</ul>
<h3>PushBox2</h3>
<h4>State variables</h4>
<ul>
<li>target-x</li>
<li>target-y</li>
<li>robot1-x</li>
<li>robot1-y</li>
<li>robot2-x</li>
<li>robot2-y</li>
<li>box-x</li>
<li>box-y</li>
<li>robot1-to-box-x</li>
<li>robot1-to-box-y</li>
<li>robot2-to-box-x</li>
<li>robot2-to-box-y</li>
<li>box-to-target-x</li>
<li>box-to-target-y</li>
<li>robot1-theta</li>
<li>robot2-theta</li>
<li>box-theta</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>robot1-v</li>
<li>robot1-omega</li>
<li>robot2-v</li>
<li>robot2-omega</li>
</ul>
<h3>RainCar</h3>
<h4>State variables</h4>
<ul>
<li>position</li>
<li>velocity</li>
<li>position-deviation</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>acceleration</li>
</ul>
<h3>RobotControl</h3>
<h4>State variables</h4>
<ul>
<li>target-x</li>
<li>target-y</li>
<li>robot1-x</li>
<li>robot1-y</li>
<li>robot1-theta</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>robot1-v</li>
<li>robot1-omega</li>
</ul>
<h3>SwingupPendulum</h3>
<h4>State variables</h4>
<ul>
<li>angle</li>
<li>angular-velocity</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>torque</li>
</ul>
<h3>UnderwaterVehicle</h3>
<h4>State variables</h4>
<ul>
<li>v-setpoint</li>
<li>v</li>
<li>v-deviation</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>u-thrust</li>
</ul>
<h3>WindTurbine</h3>
<h4>State variables</h4>
<ul>
<li>T_a</li>
<li>P_a</li>
<li>P_s</li>
<li>P_e</li>
<li>E_p</li>
<li>v</li>
<li>omega_r</li>
<li>d_omega_r</li>
<li>E_omega_r</li>
<li>omega_g</li>
<li>d_omega_g</li>
<li>E_omega_g</li>
<li>beta</li>
<li>d_beta</li>
<li>T_g</li>
<li>d_T_g</li>
<li>E_int_omega_r</li>
<li>E_int_omega_g</li>
<li>theta</li>
</ul>
<h4>Action variables</h4>
<ul>
<li>beta</li>
<li>T_g</li>
</ul>
</body>
</html>
