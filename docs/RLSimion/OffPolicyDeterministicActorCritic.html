<html>
<body>
<h1>Class OffPolicyDeterministicActorCritic</h1>
<em>Source: actor-critic-opdac.cpp</em>
<h2>Methods</h2>
<h3><code>double update(const State *s, const Action *a, const State *s_p, double r, double behaviorProb)</code></h3>
<ul>
<li><b>Summary</b></li>
<p>Updates the policy and the value function using the Incremental Natural Actor Critic algorithm in "Off-policy deterministic actorcritic (OPDAC)" in "Deterministic Policy Gradient Algorithms" (David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller). Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: WCP volume 32</p>
<li><b>Parameters</b></li>
<ul>
<li><i>s</i>: Initial state</li>
<li><i>a</i>: Action</li>
<li><i>s_p</i>: Resultant state</li>
<li><i>r</i>: Reward</li>
<li><i>behaviorProb</i>: Probability by which the actor selected the action</li>
</ul>
<li><b>Return Value</b></li>
<p>Updates the policy and the value function using the Incremental Natural Actor Critic algorithm in "Off-policy deterministic actorcritic (OPDAC)" in "Deterministic Policy Gradient Algorithms" (David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller). Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: WCP volume 32</p>
</ul>
<h3><code>double selectAction(const State *s, Action *a)</code></h3>
<ul>
<li><b>Summary</b></li>
<p>The actor selects an action following the policies it is learning</p>
<li><b>Parameters</b></li>
<ul>
<li><i>s</i>: Initial state</li>
<li><i>a</i>: Action</li>
</ul>
<li><b>Return Value</b></li>
<p>The actor selects an action following the policies it is learning</p>
</ul>
</body>
</html>
